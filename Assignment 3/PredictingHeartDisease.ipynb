{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2ca8abf3-e940-4a77-97a4-6f2f69789236",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f0ca87-d09c-4e95-aedd-fd4d81666de4",
   "metadata": {},
   "source": [
    "## Part A ##\n",
    "**Read the data file “Hearts_s.csv” (from github using the following command), and assign it to a Pandas DataFrame**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "65db2e58-8bc7-4239-8a83-4713911f831d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"https://github.com/mpourhoma/CS4661/raw/master/Heart_s2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ffcf4197-d829-4cbd-81f8-64554854e189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>ChestPain</th>\n",
       "      <th>RestBP</th>\n",
       "      <th>Chol</th>\n",
       "      <th>RestECG</th>\n",
       "      <th>MaxHR</th>\n",
       "      <th>Oldpeak</th>\n",
       "      <th>Thal</th>\n",
       "      <th>AHD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>typical</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>2.3</td>\n",
       "      <td>fixed</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67</td>\n",
       "      <td>asymptomatic</td>\n",
       "      <td>160</td>\n",
       "      <td>286</td>\n",
       "      <td>2</td>\n",
       "      <td>108</td>\n",
       "      <td>1.5</td>\n",
       "      <td>normal</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>67</td>\n",
       "      <td>asymptomatic</td>\n",
       "      <td>120</td>\n",
       "      <td>229</td>\n",
       "      <td>2</td>\n",
       "      <td>129</td>\n",
       "      <td>2.6</td>\n",
       "      <td>reversable</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37</td>\n",
       "      <td>nonanginal</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>187</td>\n",
       "      <td>3.5</td>\n",
       "      <td>normal</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>nontypical</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>2</td>\n",
       "      <td>172</td>\n",
       "      <td>1.4</td>\n",
       "      <td>normal</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age     ChestPain  RestBP  Chol  RestECG  MaxHR  Oldpeak        Thal  AHD\n",
       "0   63       typical     145   233        2    150      2.3       fixed   No\n",
       "1   67  asymptomatic     160   286        2    108      1.5      normal  Yes\n",
       "2   67  asymptomatic     120   229        2    129      2.6  reversable  Yes\n",
       "3   37    nonanginal     130   250        0    187      3.5      normal   No\n",
       "4   41    nontypical     130   204        2    172      1.4      normal   No"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f0441b-96d7-4d22-aae6-5846aadd9957",
   "metadata": {},
   "source": [
    "## Part C ##\n",
    "**As you see, there are several categorical features in the dataset (ChestPain, Thal). Let’s ignore these categorical features for now, and only keep the numerical features and build your feature matrix and label vector.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d9a6135f-9438-49e8-b34a-e3f6244d8e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Age  RestBP  Chol  RestECG  MaxHR  Oldpeak\n",
      "0     63     145   233        2    150      2.3\n",
      "1     67     160   286        2    108      1.5\n",
      "2     67     120   229        2    129      2.6\n",
      "3     37     130   250        0    187      3.5\n",
      "4     41     130   204        2    172      1.4\n",
      "..   ...     ...   ...      ...    ...      ...\n",
      "296   45     110   264        0    132      1.2\n",
      "297   68     144   193        0    141      3.4\n",
      "298   57     130   131        0    115      1.2\n",
      "299   57     130   236        2    174      0.0\n",
      "300   38     138   175        0    173      0.0\n",
      "\n",
      "[301 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "features = df.drop(columns=[\"ChestPain\", \"Thal\", \"AHD\"])\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "71b9d127-796d-4561-99f9-ab255bf4065c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       No\n",
      "1      Yes\n",
      "2      Yes\n",
      "3       No\n",
      "4       No\n",
      "      ... \n",
      "296    Yes\n",
      "297    Yes\n",
      "298    Yes\n",
      "299    Yes\n",
      "300     No\n",
      "Name: AHD, Length: 301, dtype: object\n"
     ]
    }
   ],
   "source": [
    "target = df[\"AHD\"]\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adf7200-978a-451e-8005-86ad810dbe08",
   "metadata": {},
   "source": [
    "## Part D ##\n",
    "**Split the dataset into testing and training sets with the following parameters: test_size=0.20, random_state=9**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "66ebc438-670c-445f-b82f-c13e86ea5479",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size = 0.20, random_state = 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b738f4-4cbf-4450-ab06-324c5ac163e6",
   "metadata": {},
   "source": [
    "## Part E ##\n",
    "**Use KNN (with k=7), Decision Tree (with random_state=5 (this random state is used when you define your decision tree classifier. It is different from the random state that you used to split the data in part D)), and Logistic Regression Classifiers (with max_iter=400) to predict Heart Disease based on the training/testing datasets that you built in part (D). Then check, compare, and report the accuracy of these 3 classifiers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "27e2d29a-748b-4acb-8793-e2f70f6dd8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Setting KNN with k = 7, decision tree with random state 5, and logistic regression with max_iter = 400\n",
    "knn = KNeighborsClassifier(n_neighbors=7)\n",
    "my_decisiontree = DecisionTreeClassifier(random_state = 5)\n",
    "my_logreg = LogisticRegression(max_iter = 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "73b4f6ef-a76e-461c-9e29-b9dbef960872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and testing KNN Classifier\n",
    "knn.fit(X_train, y_train)\n",
    "knn_predictions = knn.predict(X_test)\n",
    "knn_accuracy = accuracy_score(y_test, knn_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "54356c28-1f81-4df3-b8f1-948fe9e6b201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and testing Decision Tree Classifier\n",
    "my_decisiontree.fit(X_train, y_train)\n",
    "decision_tree_predictions = my_decisiontree.predict(X_test)\n",
    "decision_tree_accuracy = accuracy_score(y_test, decision_tree_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ec033a2f-f8a0-47dc-bfaa-c048a08fcd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and testing Logistic Regression Classifier\n",
    "my_logreg.fit(X_train, y_train)\n",
    "logistic_regression_predictions = my_logreg.predict(X_test)\n",
    "logistic_regression_accuracy = accuracy_score(y_test, logistic_regression_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a9d769cf-c215-4fd0-9a14-8831762a2beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Classifier Accuracy: 0.6229508196721312\n",
      "Decision Tree Classifier Accuracy: 0.6885245901639344\n",
      "Logistic Regression Classifier Accuracy: 0.7213114754098361\n"
     ]
    }
   ],
   "source": [
    "# Printing classifier accuracies\n",
    "print(\"KNN Classifier Accuracy:\", knn_accuracy)\n",
    "print(\"Decision Tree Classifier Accuracy:\", decision_tree_accuracy)\n",
    "print(\"Logistic Regression Classifier Accuracy:\", logistic_regression_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e579ba8-ea89-49ce-a15b-44aae69fa124",
   "metadata": {},
   "source": [
    "**Which one is the best? Which one is the worst?**\n",
    "\n",
    "The one with the best accuracy is Logistic Regression anf the worst accuracy is the KNN classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d8f2c7-5f8e-42ca-8e2b-d243acf704b0",
   "metadata": {},
   "source": [
    "## Part F ##\n",
    "**Now, we want to use the categorical features as well! To this end, we have to perform a feature engineering process called OneHotEncoding for the categorical features. To do this, each categorical feature should be replaced with dummy columns in the feature table (one column for each possible value of a categorical feature), and then encode it in a binary manner such that only one of the dummy columns can take “1” at a time (and zero for the rest). For example, “Thal” can take three values “fixed” and “normal” and \"reversable\". Thus, we need to replace this feature (in the feature table) with 3 columns titled “fixed”, “normal”, and \"reversable\".  Wherever we have a value \"fixed\", we should put “1”, ”0”, \"0\" in the columns “fixed” and “normal” and \"reversable\".  (Hint: Similarly, you will need 4 columns to encode “ChestPain”)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "bc1d8098-9304-42ef-91ab-602fc0fca332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Age  RestBP  Chol  RestECG  MaxHR  Oldpeak  AHD  Thal_fixed  Thal_normal  \\\n",
      "0   63     145   233        2    150      2.3   No        True        False   \n",
      "1   67     160   286        2    108      1.5  Yes       False         True   \n",
      "2   67     120   229        2    129      2.6  Yes       False        False   \n",
      "3   37     130   250        0    187      3.5   No       False         True   \n",
      "4   41     130   204        2    172      1.4   No       False         True   \n",
      "\n",
      "   Thal_reversable  ChestPain_asymptomatic  ChestPain_nonanginal  \\\n",
      "0            False                   False                 False   \n",
      "1            False                    True                 False   \n",
      "2             True                    True                 False   \n",
      "3            False                   False                  True   \n",
      "4            False                   False                 False   \n",
      "\n",
      "   ChestPain_nontypical  ChestPain_typical  \n",
      "0                 False               True  \n",
      "1                 False              False  \n",
      "2                 False              False  \n",
      "3                 False              False  \n",
      "4                  True              False  \n"
     ]
    }
   ],
   "source": [
    "# Perform One-Hot Encoding for the \"Thal\" feature\n",
    "Thal_encoded = pd.get_dummies(df['Thal'], prefix='Thal')\n",
    "# Replacing the old Thal column with One Hot Encoded Thal column\n",
    "df = pd.concat([df.drop('Thal', axis=1), Thal_encoded], axis=1)\n",
    "\n",
    "# Perform One-Hot Encoding for the \"ChestPain\" feature\n",
    "ChestPain_encoded = pd.get_dummies(df['ChestPain'], prefix='ChestPain')\n",
    "# Replacing the old ChestPain column with One Hot Encoded Chest Pain column\n",
    "df = pd.concat([df.drop('ChestPain', axis=1), ChestPain_encoded], axis=1)\n",
    "\n",
    "# Printing new data frame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18568e9a-b39b-440e-95e9-2ad3a9c80f4f",
   "metadata": {},
   "source": [
    "## Part G ##\n",
    "**Repeat parts (d) and (e) with the new dataset that you built in part (f). How does the prediction accuracy change for each method?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3111104e-9643-4b51-bcad-af9da3919ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Features\n",
    "features = df.drop(columns=[\"AHD\"])\n",
    "\n",
    "# Target variable\n",
    "target = df[\"AHD\"]\n",
    "\n",
    "# Split the dataset into testing and training sets with the following parameters\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.20, random_state=9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3ec30a81-eb0e-43d8-aeb3-14e7dfff08ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Classifier Accuracy (After One-Hot Encoding): 0.6229508196721312\n",
      "Decision Tree Classifier Accuracy (After One-Hot Encoding): 0.7377049180327869\n",
      "Logistic Regression Classifier Accuracy (After One-Hot Encoding): 0.7868852459016393\n"
     ]
    }
   ],
   "source": [
    "# Initializing classifiers\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=7)\n",
    "decision_tree_classifier = DecisionTreeClassifier(random_state=5)\n",
    "logistic_regression_classifier = LogisticRegression(max_iter=400)\n",
    "\n",
    "# Training and testing KNN Classifier\n",
    "knn_classifier.fit(X_train, y_train)\n",
    "knn_predictions = knn_classifier.predict(X_test)\n",
    "knn_accuracy = accuracy_score(y_test, knn_predictions)\n",
    "\n",
    "# Training and testing Decision Tree Classifier\n",
    "decision_tree_classifier.fit(X_train, y_train)\n",
    "decision_tree_predictions = decision_tree_classifier.predict(X_test)\n",
    "decision_tree_accuracy = accuracy_score(y_test, decision_tree_predictions)\n",
    "\n",
    "# Training and testing Logistic Regression Classifier\n",
    "logistic_regression_classifier.fit(X_train, y_train)\n",
    "logistic_regression_predictions = logistic_regression_classifier.predict(X_test)\n",
    "logistic_regression_accuracy = accuracy_score(y_test, logistic_regression_predictions)\n",
    "\n",
    "# Printing accuracies\n",
    "print(\"KNN Classifier Accuracy (After One-Hot Encoding):\", knn_accuracy)\n",
    "print(\"Decision Tree Classifier Accuracy (After One-Hot Encoding):\", decision_tree_accuracy)\n",
    "print(\"Logistic Regression Classifier Accuracy (After One-Hot Encoding):\", logistic_regression_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9da5b44d-d4e5-4452-9979-007778de5072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Classifier Accuracy (After One-Hot Encoding and 10-fold CV): 0.6711827956989247\n",
      "Decision Tree Classifier Accuracy (After One-Hot Encoding and 10-fold CV): 0.7308602150537634\n",
      "Logistic Regression Classifier Accuracy (After One-Hot Encoding and 10-fold CV): 0.7906451612903226\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "# classifiers\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=7)\n",
    "decision_tree_classifier = DecisionTreeClassifier(random_state=5)\n",
    "logistic_regression_classifier = LogisticRegression(max_iter=400)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category = ConvergenceWarning)\n",
    "\n",
    "# 10-fold Cross-Validation and compute accuracies\n",
    "knn_cv_scores = cross_val_score(knn_classifier, features, target, cv=10, scoring='accuracy')\n",
    "decision_tree_cv_scores = cross_val_score(decision_tree_classifier, features, target, cv=10, scoring='accuracy')\n",
    "logistic_regression_cv_scores = cross_val_score(logistic_regression_classifier, features, target, cv=10, scoring='accuracy')\n",
    "\n",
    "# Printing Cross-Validation accuracies\n",
    "print(\"KNN Classifier Accuracy (After One-Hot Encoding and 10-fold CV):\", knn_cv_scores.mean())\n",
    "print(\"Decision Tree Classifier Accuracy (After One-Hot Encoding and 10-fold CV):\", decision_tree_cv_scores.mean())\n",
    "print(\"Logistic Regression Classifier Accuracy (After One-Hot Encoding and 10-fold CV):\", logistic_regression_cv_scores.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29116e1-8383-448d-af80-19336857ec77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
